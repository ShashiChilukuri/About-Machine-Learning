{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "## Part 1 - Introduction to PyTorch framework\n",
    "**Author:** Shashi Kiran Chilukuri\n",
    "\n",
    "\n",
    "![title](img/pytorch.jpeg)\n",
    "\n",
    "We have many frameworks to create deep learning models, to name some: Tensorflow, Pytorch, Caffe, MXNet, Keras etc. Each of them have their own architecture style and of course each one with some pros and cons. In this 3 part article series, we will go through one of the popular and highly adopting framework \"PyTorch\". The goal of this article series is to introduce to\n",
    "   * Part 1: PyTorch framework\n",
    "   * Part 2: Tensor Operations\n",
    "   * Part 3: General steps to create neural network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Introduce to PyTorch framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So,what is PyTorch?\n",
    " ![title](img/what is Pytorch.png)\n",
    " \n",
    " Now lets compare with other most popular framework 'TensorFlow' to check how it standout\n",
    " \n",
    "## Comparing with other most popular framework: Tensorflow**\n",
    " ![title](img/comparing with TensorFlow.png)\n",
    " \n",
    " We can clearly see PyTorch out performs TensorFlow in some areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Components of Pytorch\n",
    "Before we jump into core components, lets quickly have a high level overview of what is Artificial Neural Network and general steps involved in creating a network\n",
    "\n",
    "<img src=\"img/ANN.png\" width=\"500\">\n",
    "\n",
    "Artificial Neural Networks(ANN) are vaguely inspired from human brain's neural structure. Just how a human brain process the information, a ANN will also receive, process and output the information. ANN will have at least 3 layers (Input, Hidden, Output layers) to process the information and also each of these layer can have any number of nodes. In the below picture, we can notice 2 input nodes in Input layer, 3 hidden nodes in Hidden layer and 1 output node in Output layer (also 1 bias unit for Input and hidden layer). \n",
    "<img src=\"img/Simple multi-layer neural network.PNG\" width=\"500\">\n",
    "\n",
    "ANN is an iterative process. What it means is, we train the network until difference between predicted and actual outcome is minimal. Meaning, we first receive the inputs, processes them in hidden layers and predict the  outcome(Y-Pred as shown in above picture). This process is called Forward propagation or Feedforward \n",
    "\n",
    " <img src=\"img/nn training.PNG\" width=\"700\">\n",
    "\n",
    "Next, we compare predict outcome(Y-Pred) with actual(Y) to check how well the ANN model performed.We then calculate the error function and gradient. Next, we perform the back propagation which is the process to spread the error to each of the weights using the chain rule. Finally, we update the weights and rerun the whole process until the model gets better output. Take a look at above neural networks training cycle\n",
    "\n",
    "Going back to our core components of PyTorch frameworks.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "![title](img/data dimensions.png)\n",
    "\n",
    "* Tensor as noted above is a multi-dimensional matrix which contain elements of a single data type. These are part of the pytorch’s torch package. \n",
    "\n",
    "> `torch` package provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix-vector multiplication, matrix-matrix multiplication, matrix-vector product and matrix product.\n",
    "\n",
    "* So why do we need to use PyTorch tensors when we have Numpy ndarrays? One good reason is, PyTorch can utilize power of GPU's to accelerate numerical calculations. Otherwise both are conceptually identical. Tensors in PyTorch and arrays in numpy will share their underlying memory locations, and changing one will change the other. We can convert a Tensor to array and vice versa very early\n",
    "\n",
    "### Computational Graphs\n",
    "\n",
    "Computational graphs are another core component of PyTorch deep learning framework. Computational graphs states the sequence of operations that occur between the variables. These chain computations in the neural network will result in output prediction. Neural networks are heavily dependent on computational graph during the training process. It helps to calculate the gradient (which is all the partial derivatives of error function with respect to weights) by efficiently applying the chain rule. \n",
    "\n",
    "PyTorch uses dynamic computational graph (as opposed to conventional static computational graph) to calculate the gradient during backpropagation. Here are  differences between static and dynamic computational graphs\n",
    " <img src=\"img/static vs dynamic.png\">\n",
    "\n",
    "Here is the example of computational graphs (by PyTorch.org) to show how backpropagation works using PyTorch's autograd package. This brings us to next topic - \"**Autograd**\"\n",
    " <img src=\"img/dynamic_graph.gif\">\n",
    " \n",
    "### AUTOGRAD\n",
    "\n",
    "PyTorch's Autograd package provides reverse automatic differentiation (backpropagation) for all operations on Tensors to calculate the gradient. since the PyTorch follow \"define by run\" dynamic framework, backpropagation will run the desired computation as opposed to specifying a static graph strucutre. \n",
    "\n",
    "If you are interested, take a look at [PyTorch.org article on Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) on how we can set an attribute to a Tensor and calculate the gradients automatically.\n",
    "\n",
    "### PyTorch Packages\n",
    "\n",
    "Here are some of the PyTorch packages that are used in creating ANN\n",
    "<img src=\"img/pytorch_packages.PNG\">\n",
    "\n",
    "Lets incorporate above PyTorch concepts into to our neural network training cycle. Here are the steps\n",
    "1. Convert the input data into torch tensors\n",
    "2. Create a model with sequence of input, output and hidden layers\n",
    "3. Perform forward propagation to generate predicted output\n",
    "4. Compare predicted output with actual\n",
    "5. Calculate error (difference between predicted vs actual) and then perform gradient decent using back propagation (with PyTorch's Autograd) to spread the error until the model gets better output\n",
    "6. Check the performance of the model with different metrics. \n",
    "7. Once the model is performing well, we can validate the model with test data\n",
    "\n",
    "Till now we have gone through theoretical overview of PyTorch framework, now lets install it and take this further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    " * For installation go to https://pytorch.org/\n",
    " * It has option of installing locally or on cloud\n",
    " ![title](img/installation.PNG)\n",
    "\n",
    "**Installing PyTorch on Windows**\n",
    " * As a pre-req, need to have access to computer with Python, NumPy, Matplotlib, Jupyter Notebook installed\n",
    " * Once you select the preferences, you will get the command to run\n",
    " * Note: CUDA is NVIDIA’s parallel computing platform. If your computer doesn’t have NVIDIA’s graphics chip. You can select ‘None’ in preferences\n",
    " \n",
    " <img src=\"img/installation step1.PNG\" width=\"500\"> \n",
    " * Once pytorch installation completed, it will check to proceed on torchvision. Say yes to continue…\n",
    " <img src=\"img/installation step2.PNG\" width=\"500\"> \n",
    "\n",
    "## Summary\n",
    "We have looked into what PyTorch is, its core components, and how to install it. In the next post, we will look into torch tensors, its operations and how to create feedforward neural network with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources and Interesting articles:**\n",
    "\n",
    "* [About Torch pacakage](https://en.wikipedia.org/wiki/Torch_(machine_learning)\n",
    "* [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "* [PyTorch vs Tensorflow](https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "* [Pytorch dynamic compuatational graphs](https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1)\n",
    "* [Automatic differentiation in PyTorch](https://openreview.net/pdf?id=BJJsrmfCZ)\n",
    "* [PyTorch articles](https://jhui.github.io/)\n",
    "* [Deep Learning Frameworks](https://www.kdnuggets.com/2017/02/anatomy-deep-learning-frameworks.html)\n",
    "* [Exploring PyTorch](https://blog.algorithmia.com/exploring-the-deep-learning-framework-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
